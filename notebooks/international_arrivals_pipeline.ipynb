{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb0a9431",
   "metadata": {},
   "source": [
    "\n",
    "# International passenger arrivals into Brazil\n",
    "\n",
    "This notebook builds the data pipeline for the dengue climate-change analysis. It streams\n",
    "ANAC's JSON extracts, filters for international arrivals into Brazil, aggregates passenger\n",
    "counts by month and region, and generates a region-by-region stacked bar chart.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49174d81",
   "metadata": {},
   "source": [
    "\n",
    "## Parameters\n",
    "\n",
    "Update the values in the next cell to change the analysis window or tweak output\n",
    "behaviour. The defaults pull the ANAC data from 2005–2025 and highlight the top six\n",
    "origin countries per Brazilian region.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31828d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import tarfile\n",
    "from json import JSONDecoder\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterator, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "plt.style.use('seaborn-v0_8-colorblind')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40b10fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Core configuration\n",
    "REPO_ROOT = Path.cwd()\n",
    "if not (REPO_ROOT / 'data').exists() and (REPO_ROOT.parent / 'data').exists():\n",
    "    REPO_ROOT = REPO_ROOT.parent\n",
    "\n",
    "DATA_ARCHIVE_PATH = REPO_ROOT / 'Dados_Estatisticos_2021_a_2030.json.tar.gz'\n",
    "RAW_DATA_DIR = REPO_ROOT / 'data' / 'raw' / 'anac'\n",
    "RAW_JSON_PATH = RAW_DATA_DIR / 'Dados_Estatisticos_2021_a_2030.json'\n",
    "PROCESSED_DIR = REPO_ROOT / 'data' / 'processed'\n",
    "REPORTS_DIR = REPO_ROOT / 'reports'\n",
    "FIGURES_DIR = REPO_ROOT / 'figures'\n",
    "\n",
    "START_YEAR = 2005\n",
    "END_YEAR = 2025  # Set to None to include all available years\n",
    "MAX_COUNTRIES_PER_REGION = 6\n",
    "\n",
    "REGION_MAP = {\n",
    "    'NORTE': 'North',\n",
    "    'NORDESTE': 'Northeast',\n",
    "    'CENTRO-OESTE': 'Central-West',\n",
    "    'SUDESTE': 'Southeast',\n",
    "    'SUL': 'South',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91401750",
   "metadata": {},
   "source": [
    "\n",
    "## Ensure the raw JSON is available\n",
    "\n",
    "The ANAC portal distributes data as `.json` files wrapped in tarballs. This cell extracts\n",
    "the `Dados_Estatisticos_2021_a_2030.json` payload into `data/raw/anac/` if it is missing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d496b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "if not RAW_JSON_PATH.exists():\n",
    "    if not DATA_ARCHIVE_PATH.exists():\n",
    "        raise FileNotFoundError(f'Archive not found: {DATA_ARCHIVE_PATH}')\n",
    "    with tarfile.open(DATA_ARCHIVE_PATH, 'r:gz') as archive:\n",
    "        archive.extractall(path=RAW_DATA_DIR)\n",
    "    print(f'Extracted {RAW_JSON_PATH.name} into {RAW_DATA_DIR}')\n",
    "else:\n",
    "    print('Raw JSON already present; skipping extraction.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b568a533",
   "metadata": {},
   "source": [
    "\n",
    "## Helpers for streaming concatenated JSON arrays\n",
    "\n",
    "The ANAC file concatenates many JSON arrays back-to-back. The helpers below parse those\n",
    "arrays incrementally so we never have to load the full 160+ MB text file into memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad297219",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DECODER = JSONDecoder()\n",
    "\n",
    "\n",
    "def iter_json_arrays(path: Path, chunk_size: int = 1_000_000) -> Iterator[Any]:\n",
    "    \"\"\"Yield each top-level JSON value from the file.\"\"\"\n",
    "    buffer = ''\n",
    "    with path.open('r', encoding='utf-8') as handle:\n",
    "        while True:\n",
    "            chunk = handle.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            buffer += chunk\n",
    "            while True:\n",
    "                buffer = buffer.lstrip()\n",
    "                if not buffer:\n",
    "                    break\n",
    "                try:\n",
    "                    value, index = DECODER.raw_decode(buffer)\n",
    "                except ValueError:\n",
    "                    break  # Need more data\n",
    "                yield value\n",
    "                buffer = buffer[index:]\n",
    "        buffer = buffer.lstrip()\n",
    "        while buffer:\n",
    "            value, index = DECODER.raw_decode(buffer)\n",
    "            yield value\n",
    "            buffer = buffer[index:].lstrip()\n",
    "\n",
    "\n",
    "def iter_anac_records(path: Path) -> Iterator[Dict[str, Any]]:\n",
    "    \"\"\"Stream individual flight records from the concatenated arrays.\"\"\"\n",
    "    for value in iter_json_arrays(path):\n",
    "        if isinstance(value, list):\n",
    "            for item in value:\n",
    "                if isinstance(item, dict):\n",
    "                    yield item\n",
    "        elif isinstance(value, dict):\n",
    "            yield value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d1d49e",
   "metadata": {},
   "source": [
    "\n",
    "## Load and filter international arrivals\n",
    "\n",
    "This step keeps only flights arriving into Brazil from foreign origin countries. It also\n",
    "records any inbound flights whose origin country is missing so we can report them later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7185a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "records: List[Dict[str, Any]] = []\n",
    "missing_origin_records: List[Dict[str, Any]] = []\n",
    "start_year = START_YEAR\n",
    "end_year = END_YEAR if END_YEAR is not None else float('inf')\n",
    "\n",
    "for raw in iter_anac_records(RAW_JSON_PATH):\n",
    "    year_str = raw.get('ANO')\n",
    "    month_str = raw.get('MES')\n",
    "    try:\n",
    "        year = int(year_str)\n",
    "        month = int(month_str)\n",
    "    except (TypeError, ValueError):\n",
    "        continue\n",
    "    if year < start_year or year > end_year:\n",
    "        continue\n",
    "    destination_country = (raw.get('AEROPORTO_DE_DESTINO_PAIS') or '').strip().upper()\n",
    "    if destination_country != 'BRASIL':\n",
    "        continue\n",
    "    destination_region_code = (raw.get('AEROPORTO_DE_DESTINO_REGIAO') or '').strip().upper()\n",
    "    if destination_region_code not in REGION_MAP:\n",
    "        continue\n",
    "    origin_country_raw = (raw.get('AEROPORTO_DE_ORIGEM_PAIS') or '').strip()\n",
    "    total_passengers = int(raw.get('PASSAGEIROS_PAGOS') or 0) + int(raw.get('PASSAGEIROS_GRATIS') or 0)\n",
    "    if not origin_country_raw:\n",
    "        missing_origin_records.append(\n",
    "            {\n",
    "                'year': year,\n",
    "                'destination_region': REGION_MAP[destination_region_code],\n",
    "                'passengers_total': total_passengers,\n",
    "            }\n",
    "        )\n",
    "        continue\n",
    "    origin_country_clean = origin_country_raw.strip()\n",
    "    origin_country_upper = origin_country_clean.upper()\n",
    "    if origin_country_upper in {'BRASIL', 'BRAZIL'}:\n",
    "        continue\n",
    "\n",
    "    records.append(\n",
    "        {\n",
    "            'year': year,\n",
    "            'month': month,\n",
    "            'destination_region': REGION_MAP[destination_region_code],\n",
    "            'origin_country': origin_country_clean.title(),\n",
    "            'passengers_total': total_passengers,\n",
    "        }\n",
    "    )\n",
    "\n",
    "if not records:\n",
    "    raise RuntimeError('No matching international arrivals were found. Check parameters.')\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df['year_month'] = pd.to_datetime(dict(year=df['year'], month=df['month'], day=1))\n",
    "\n",
    "print(f'Loaded {len(df):,} international arrival records spanning {df.year.min()}–{df.year.max()}.')\n",
    "display(df.head())\n",
    "\n",
    "missing_df = pd.DataFrame(missing_origin_records)\n",
    "if missing_df.empty:\n",
    "    missing_summary = pd.DataFrame(columns=['year', 'destination_region', 'passengers_total'])\n",
    "    print('No inbound records with missing origin country were found.')\n",
    "else:\n",
    "    missing_summary = (\n",
    "        missing_df.groupby(['year', 'destination_region'], as_index=False)['passengers_total']\n",
    "        .sum()\n",
    "        .sort_values(['year', 'destination_region'])\n",
    "    )\n",
    "    display(missing_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1684ac65",
   "metadata": {},
   "source": [
    "\n",
    "## Monthly aggregation and export\n",
    "\n",
    "Aggregate paid + complimentary passengers by calendar month, Brazilian region, and\n",
    "origin country. The CSV output feeds downstream epidemiological models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f919cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "monthly_passengers = (\n",
    "    df.groupby(['year_month', 'year', 'month', 'destination_region', 'origin_country'], as_index=False)\n",
    "    ['passengers_total']\n",
    "    .sum()\n",
    "    .sort_values(['year_month', 'destination_region', 'passengers_total'], ascending=[True, True, False])\n",
    ")\n",
    "\n",
    "monthly_path = PROCESSED_DIR / 'monthly_international_arrivals_by_region_origin.csv'\n",
    "monthly_passengers.to_csv(monthly_path, index=False)\n",
    "print(f'Saved monthly totals to {monthly_path}')\n",
    "\n",
    "yearly_path = PROCESSED_DIR / 'yearly_international_arrivals_by_region_origin.csv'\n",
    "\n",
    "missing_path = REPORTS_DIR / 'missing_origin_summary.csv'\n",
    "missing_summary.to_csv(missing_path, index=False)\n",
    "print(f'Logged missing-origin summary to {missing_path}')\n",
    "\n",
    "display(monthly_passengers.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71753375",
   "metadata": {},
   "source": [
    "\n",
    "## Yearly view and stacked bar charts\n",
    "\n",
    "To support presentation needs, collapse the data to yearly totals and keep only the top\n",
    "`MAX_COUNTRIES_PER_REGION` origin countries for each Brazilian region. The remaining\n",
    "countries are grouped under “Other” to keep the stacked bars readable. The figure is\n",
    "saved to `figures/international_arrivals_by_region.png`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbfbe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_df = df.copy()\n",
    "region_country_totals = plot_df.groupby(['destination_region', 'origin_country'], as_index=False)['passengers_total'].sum()\n",
    "region_top_countries = (\n",
    "    region_country_totals.sort_values(['destination_region', 'passengers_total'], ascending=[True, False])\n",
    "    .groupby('destination_region')\n",
    "    .head(MAX_COUNTRIES_PER_REGION)\n",
    ")\n",
    "allowed_countries = {region: set(group['origin_country']) for region, group in region_top_countries.groupby('destination_region')}\n",
    "\n",
    "plot_df['origin_country_grouped'] = plot_df.apply(\n",
    "    lambda row: row['origin_country'] if row['origin_country'] in allowed_countries.get(row['destination_region'], set()) else 'Other',\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "yearly_totals = plot_df.groupby(['year', 'destination_region', 'origin_country_grouped'], as_index=False)['passengers_total'].sum()\n",
    "yearly_totals.to_csv(yearly_path, index=False)\n",
    "print(f'Saved yearly totals to {yearly_path}')\n",
    "\n",
    "display(yearly_totals.head())\n",
    "\n",
    "regions_order = ['North', 'Northeast', 'Central-West', 'Southeast', 'South']\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 12), sharex=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, region in enumerate(regions_order):\n",
    "    ax = axes[idx]\n",
    "    region_slice = yearly_totals[yearly_totals['destination_region'] == region]\n",
    "    if region_slice.empty:\n",
    "        ax.set_visible(False)\n",
    "        continue\n",
    "    pivot = region_slice.pivot(index='year', columns='origin_country_grouped', values='passengers_total').fillna(0)\n",
    "    pivot.sort_index(inplace=True)\n",
    "    countries = list(pivot.columns)\n",
    "    color_cycle = [plt.get_cmap('tab20')(i % 20) for i in range(len(countries))]\n",
    "    pivot.plot(kind='bar', stacked=True, ax=ax, color=color_cycle)\n",
    "    ax.set_title(region)\n",
    "    ax.set_ylabel('Passengers')\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(1.02, 1.0))\n",
    "\n",
    "for j in range(len(regions_order), len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "fig.suptitle('International passenger arrivals into Brazil by destination region and origin country')\n",
    "fig.tight_layout(rect=[0, 0, 0.85, 0.96])\n",
    "figure_path = FIGURES_DIR / 'international_arrivals_by_region.png'\n",
    "fig.savefig(figure_path, dpi=300)\n",
    "print(f'Saved figure to {figure_path}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d522674d",
   "metadata": {},
   "source": [
    "\n",
    "## Next steps\n",
    "\n",
    "- Push the processed CSVs, report, and figure so collaborators can use them without\n",
    "  downloading the raw 160 MB JSON.\n",
    "- When the 2000–2020 archives become available locally, point `DATA_ARCHIVE_PATH` at the\n",
    "  new tarball and widen the `START_YEAR`/`END_YEAR` window to regenerate the full\n",
    "  2005–2025 series.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
